ts$created_at <- ymd_hms(ts$created_at)
#define start and end dates of the time frame
start_date <- ymd("2025-01-01")
end_date <- ymd("2025-09-26")
#filter df
ts1 <- ts %>% filter(created_at >= start_date, created_at <= end_date)
#filter df for blank content
ts1 <- ts1 %>% filter(trimws(content) != '')
#filter df for retweets and links due to retweeting
ts1 <- ts1 %>% filter(!str_detect(content, pattern = c("https|RT: ")))
#S&P500
sp500 <- sp500 %>%
mutate(
#finance features
daily_change = Close - Open, #add a change in price per day
daily_volatility = High - Low,
daily_return = (Close - lag(Close)) / lag(Close), #pct change day by day
sma20 = SMA(Close, n = 20), #20 day simple moving avg
#time based features
day_of_week = wday(Date, label = TRUE, abbr = FALSE), #add a day of week label
month = month(Date, label = TRUE, abbr = FALSE) #add a month label
)
#add a movement feature
sp500 <- sp500 %>%
mutate(
Result = case_when(
daily_change > 0 ~ "gain",
daily_change < 0 ~ "loss",
daily_change == 0 ~ "none",
TRUE ~ NA_character_ #fallback for NA values
)
)
#NASDAQ
nasdaq <- nasdaq %>%
mutate(
#finance features
daily_change = Close - Open, #add a change in price per day
daily_volatility = High - Low,
daily_return = (Close - lag(Close)) / lag(Close), #pct change day by day
sma20 = SMA(Close, n = 20), #20 day simple moving avg
#time based features
day_of_week = wday(Date, label = TRUE, abbr = FALSE), #add a day of week label
month = month(Date, label = TRUE, abbr = FALSE) #add a month label
)
#add a movement feature
nasdaq <- nasdaq %>%
mutate(
Result = case_when(
daily_change > 0 ~ "gain",
daily_change < 0 ~ "loss",
daily_change == 0 ~ "none",
TRUE ~ NA_character_ #fallback for NA values
)
)
ts1_vader_sentiment <- vader_df(ts1$content) # apply vader lexicon based sentiment analysis
ts1 <- cbind(ts1, ts1_vader_sentiment) #join sentiment results back to original df
ts1 <- ts1 %>%
mutate(sentiment_class = case_when(
compound >= 0.05 ~ "Positive",
compound > -0.05 & compound < 0.05 ~ "Neutral",
compound <= -0.05 ~ "Negative",
TRUE ~ NA_character_ # Handle any unexpected cases
))
vader_sentiment_counts <- ts1 %>%
count(sentiment_class)
#TS
ts1 <- ts1 %>% mutate(
post_date = as_date(created_at),
post_time = hms::as_hms(created_at)
)
ts1 <- ts1 %>%
mutate(
sentiment_date = if_else(
post_time < hms::as_hms("16:30:00"),
post_date,
#if after 4:30 PM, calculate the next business day
case_when(
wday(post_date) == 6 ~ post_date + 3, # Friday -> Monday (+3)
wday(post_date) == 7 ~ post_date + 2, # Saturday -> Monday (+2)
TRUE ~ post_date + 1 # All other days -> next day (+1)
)
)
)
indices <- sp500 %>%
inner_join(nasdaq,
by = 'Date',
suffix = c('_sp500','_nasdaq')
)
#Join indices with sentiment data
indices_sent <- indices %>%
left_join(ts1, by=c('Date' =  'sentiment_date'))
indices_union <- bind_rows(
"sp500" = sp500,
"nasdaq" = nasdaq,
.id = "source"
)
indices_long <- indices_union %>%
pivot_longer(
cols = where(is.numeric),
names_to = "metric",
values_to = "value"
)
p1 <- ggplot(indices_long, aes(x = metric, y = value, fill = source)) +
geom_boxplot() +
labs(
title = "Distribution of Metrics by Index",
x = "Metric",
y = "Value",
fill = "Source Index"
) +
# Rotate axis labels for better readability
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot(p1)
#sp500
sp500_long <- sp500 %>%
pivot_longer(
cols = where(is.numeric),
names_to = "metric",
values_to = "value"
)
p2 <- ggplot(sp500_long, aes(x = metric, y = value)) +
geom_boxplot() +
labs(
title = "Distribution of Metrics by Index",
x = "Metric",
y = "Value"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot(p2)
#nasdaq
nasdaq_long <- nasdaq %>%
pivot_longer(
cols = where(is.numeric),
names_to = "metric",
values_to = "value"
)
p3 <- ggplot(nasdaq_long, aes(x = metric, y = value)) +
geom_boxplot() +
labs(
title = "Distribution of Metrics by Index",
x = "Metric",
y = "Value"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot(p3)
#Normalize indices\$Close_i for i in {\_sp500, nasdaq}
indices_normalized <- indices %>%
mutate(
Close_sp500_normalized = (Close_sp500 / first(Close_sp500)) * 100,
Close_nasdaq_normalized = (Close_nasdaq / first(Close_nasdaq)) * 100
)
p4 <- ggplot(data = indices_normalized, aes(x = Date)) +
geom_line(aes(y = Close_sp500_normalized, color = "S&P 500"), linewidth = 1) +
geom_line(aes(y = Close_nasdaq_normalized, color = "NASDAQ"), linewidth = 1) +
labs(
title = "S&P 500 vs. NASDAQ Performance",
subtitle = "Indexed to start date (Base = 100)",
y = "Normalized Price",
x = "Date",
color = "Index"
) +
theme_minimal() +
theme(legend.position = "bottom")
plot(p4)
daily_avg_change <- indices %>%
group_by(day_of_week_sp500) %>%
summarise(
avg_change_sp500 = mean(daily_change_sp500, na.rm = TRUE),
avg_change_nasdaq = mean(daily_change_nasdaq, na.rm = TRUE),
.groups = "drop" #drop the grouping to prevent errors in later steps
)
#pivot the data to a tidy format for plotting both indices
daily_avg_change_tidy <- daily_avg_change %>%
pivot_longer(
cols = c(avg_change_sp500, avg_change_nasdaq),
names_to = "index",
values_to = "average_change"
)
#plot barchart
p5 <- ggplot(daily_avg_change_tidy, aes(x = day_of_week_sp500, y = average_change, fill = index)) +
geom_col(position = "dodge") +
labs(
title = "Average Daily Change by Day of Week",
subtitle = "S&P 500 vs. NASDAQ",
x = "Day of Week",
y = "Average Change",
fill = "Index"
) +
theme_minimal() +
theme(legend.position = "bottom")
plot(p5)
#get posts per day from ts1
ppd <- ts1 %>%
mutate(Date = as_date(created_at)) %>%
group_by(Date) %>%
summarise(post_count = n()) %>%
ungroup()
#combine ppd and indicies data
ppd_nasdaq <- nasdaq %>%
left_join(ppd, by = 'Date') %>%
mutate(post_count = replace_na(post_count, 0))
#combine ppd and indicies data
ppd_nasdaq <- nasdaq %>%
left_join(ppd, by = 'Date') %>%
mutate(post_count = replace_na(post_count, 0))
#scale
scale_factor <- max(ppd_nasdaq$Close, na.rm = TRUE) / max(ppd_nasdaq$post_count, na.rm = TRUE)
#plot nasdaq
p6 <- ggplot(ppd_nasdaq, aes(x = Date)) +
geom_line(aes(y = Close, color = "Closing Price"), linewidth = 1) +
geom_col(aes(y = post_count * scale_factor, fill = "Tweet Count"), alpha = 0.5) +
scale_y_continuous(
name = "Stock Price (USD)",
# Add a secondary axis for the tweet count, using the scaling factor
sec.axis = sec_axis(~ . / scale_factor, name = "Count of Tweets")
) +
scale_color_manual(name = "Data", values = "darkblue") +
scale_fill_manual(name = "Data", values = "coral") +
labs(
title = "NASDAQ Closing Price vs. Tweet Count",
x = "Date",
color = "Data",
fill = "Data"
) +
theme_minimal() +
theme(legend.position = "bottom")
#combine ppd and indicies data
ppd_sp500 <- sp500 %>%
left_join(ppd, by = 'Date') %>%
mutate(post_count = replace_na(post_count, 0))
#scale
scale_factor <- max(ppd_sp500$Close, na.rm = TRUE) / max(ppd_sp500$post_count, na.rm = TRUE)
#plot sp500
p7 <- ggplot(ppd_sp500, aes(x = Date)) +
geom_line(aes(y = Close, color = "Closing Price"), linewidth = 1) +
geom_col(aes(y = post_count * scale_factor, fill = "Tweet Count"), alpha = 0.5) +
scale_y_continuous(
name = "Stock Price (USD)",
# Add a secondary axis for the tweet count, using the scaling factor
sec.axis = sec_axis(~ . / scale_factor, name = "Count of Tweets")
) +
scale_color_manual(name = "Data", values = "darkblue") +
scale_fill_manual(name = "Data", values = "coral") +
labs(
title = "S&P500 Closing Price vs. Tweet Count",
x = "Date",
color = "Data",
fill = "Data"
) +
theme_minimal() +
theme(legend.position = "bottom")
plot(p6)
plot(p7)
p8 <- ggplot(vader_sentiment_counts, aes(x = sentiment_class, y = n, fill = sentiment_class)) +
geom_bar(stat = "identity") +
labs(
title = "Sentiment Distribution of Social Media Posts",
x = "Sentiment",
y = "Number of Posts"
) +
theme_minimal()
plot(p8)
#find the avg sentiment per day from djt's posts
daily_sentiment <- ts1 %>%
group_by(Date = as_date(created_at)) %>%
summarise(
avg_sentiment = mean(compound, na.rm=TRUE)
)
#merge with sp500
avg_sent_sp500 <- left_join(sp500, daily_sentiment, by=c("Date" = "Date"))
#filter out content = c(retweets)
#use grepl
price_range <- max(avg_sent_sp500$Close, na.rm = TRUE) - min(avg_sent_sp500$Close, na.rm = TRUE)
sentiment_range <- max(avg_sent_sp500$avg_sentiment, na.rm = TRUE) - min(avg_sent_sp500$avg_sentiment, na.rm = TRUE)
scale_factor <- price_range / sentiment_range
p9 <- ggplot(avg_sent_sp500, aes(x = Date)) +
geom_line(aes(y = Close), color = "steelblue", size = 1) +
geom_line(aes(y = avg_sentiment * scale_factor), color = "darkred", size = 1) +
scale_y_continuous(
name = "S&P 500 Closing Price",
#add the second axis, using the inverse of the scaling factor
sec.axis = sec_axis(~./scale_factor, name="Average VADER Sentiment")
) +
labs(
title = "S&P 500 Closing Price vs. Average Daily Sentiment",
x = "Date"
) +
theme_minimal() +
#customize axis colors for clarity
theme(
axis.title.y = element_text(color = "steelblue"),
axis.text.y = element_text(color = "steelblue"),
axis.title.y.right = element_text(color = "darkred"),
axis.text.y.right = element_text(color = "darkred")
)
plot(p9)
indices_sent_long <- indices_sent %>%
pivot_longer(
cols = c(Close_sp500, Close_nasdaq),
names_to = "metric",
values_to = "values"
)
p10 <- ggplot(indices_sent_long, aes(x = sentiment_class, y = values, fill = sentiment_class)) +
geom_boxplot() +
labs(
title = "Index Closing Prices by Sentiment Class",
x = "Tweet Sentiment Class",
y = "Closing Price (USD)"
) +
# Use facet_wrap to create separate plots for S&P 500 and Nasdaq
facet_wrap(~ metric, scales = "free_y") +
theme_minimal()
plot(p10)
indices_sent_long <- indices_sent %>%
pivot_longer(
cols = c(daily_change_sp500, daily_change_nasdaq),
names_to = "metric",
values_to = "values"
)
p11 <- ggplot(indices_sent_long, aes(x = sentiment_class, y = values, fill = sentiment_class)) +
geom_boxplot() +
labs(
title = "Index Closing Prices by Sentiment Class",
x = "Tweet Sentiment Class",
y = "Daily Change in Price (USD)"
) +
# Use facet_wrap to create separate plots for S&P 500 and Nasdaq
facet_wrap(~ metric, scales = "free_y") +
theme_minimal()
plot(p11)
indices_sent_long <- indices_sent %>%
pivot_longer(
cols = c(daily_change_sp500, daily_change_nasdaq),
names_to = "metric",
values_to = "values"
)
indices_sent_long_filtered <- indices_sent_long %>%
filter(!str_detect(content, pattern = c("https|RT: ")))
p12 <- ggplot(indices_sent_long_filtered, aes(x = sentiment_class, y = values, fill = sentiment_class)) +
geom_boxplot() +
labs(
title = "Index Closing Prices by Sentiment Class",
x = "Tweet Sentiment Class",
y = "Daily Change in Price Price (USD)"
) +
# Use facet_wrap to create separate plots for S&P 500 and Nasdaq
facet_wrap(~ metric, scales = "free_y") +
theme_minimal()
plot(p12)
#Assign a 'doc_id' to ts1 to add back the topics after they have been discovered by topic model
ts1$doc_id <- as.character(1:nrow(ts1))
#create vcorpus
trump_vcorpus <- VCorpus(VectorSource((ts1$content)))
#Cleaning
trump_vcorpus <- tm_map(trump_vcorpus, content_transformer(tolower))
trump_vcorpus <- tm_map(trump_vcorpus, removePunctuation) #remove punctuation
#trump_vcorpus <- tm_map(trump_vcorpus, removeNumbers)     #remove numbers
trump_vcorpus <- tm_map(trump_vcorpus, removeWords, tm::stopwords('english'))
#trump_vcorpus <- tm_map(trump_vcorpus, stripWhitespace)   #remove dupe whire space
trump_qeda_corpus <- corpus(trump_vcorpus)
trump_dfm <- dfm(tokens(trump_qeda_corpus))
word_frequencies <- textstat_frequency(trump_dfm)
print(word_frequencies)
p13 <- trump_dfm %>%
textstat_frequency(n=30) %>%
ggplot(aes(x=reorder(feature,frequency),y=frequency)) +
geom_point() +
coord_flip() +
labs(
x = NULL,
y = "Frequency"
) +
theme_minimal()
plot(p13)
tokens_trump <- tokens(trump_qeda_corpus, remove_symbols = TRUE)
tstat_col_trump <- tokens_select(
tokens_trump,
pattern = "^[0-9A-Z]",
valuetype = "regex",
case_insensitive = TRUE,
padding = TRUE
) %>% textstat_collocations(
min_count = 10,
size = 4
)
head(tstat_col_trump, 20)
trump_dtm = DocumentTermMatrix(trump_vcorpus)
inspect(trump_dtm)
findFreqTerms(trump_dtm, 15)
# try against dictionary
#inspect(DocumentTermMatrix(trump_vcorpus), list(dictionary = c()))
set.seed(123) #set seed for reproducability
trump_lda <- LDA(
trump_dtm,
k = 10,
control = list(seed = 123)
)
View(p111)
View(p111)
#get top n words for each topic
trump_topic_terms <- terms(trump_lda, 10)
print(trump_topic_terms)
#plot the topic distributions
topic_dists_trump <- tidy(
trump_lda,
matrix = "gamma"
)
trump_max_topics <- topic_dists_trump %>%
group_by(document) %>%
summarise(max_gamma = max(gamma)) %>%
ungroup()
selected_documents <- trump_max_topics %>%
filter(max_gamma < 0.8) %>%
top_n(12, max_gamma) %>% # Select the 12 most "mixed" (or a different number)
pull(document)
plot_data <- topic_dists_trump %>%
filter(document %in% selected_documents) %>%
# Reorder the documents for better display in the plot
mutate(document = factor(document, levels = selected_documents))
ggplot(plot_data, aes(x = topic, y = gamma, fill = topic)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ document, scales = "free_y", ncol = 4) + # ncol sets the number of columns
scale_y_continuous(labels = scales::percent) +
labs(
title = "Topic Distribution Across Selected Documents",
subtitle = "Document-Topic Proportion (Gamma)",
x = "Topic Number",
y = "Proportion"
) +
theme_minimal() #+
#theme(axis.text.x = element_blank())
ggplot(plot_data, aes(x = topic, y = gamma, color = topic)) +
geom_point(size = 3, show.legend = FALSE) +
facet_wrap(~ document, scales = "free_y", ncol = 4) +
scale_y_continuous(labels = scales::percent) +
labs(
title = "Topic Distribution Across Selected Documents",
subtitle = "Document-Topic Proportion (Gamma)",
x = "Topic Number",
y = "Proportion"
) +
theme_minimal()
#assign topic categories
#get topic distribution for documents
trump_topic_dominant <- topic_dists_trump %>%
group_by(document) %>%
slice_max(gamma, n = 1) %>%
ungroup()
#check if the left join has already occured
if (!("topic" %in% names(ts1))) {
# If the column does not exist, run the left_join
ts1 <- left_join(ts1, trump_topic_dominant, by = c('doc_id' = 'document'))
}
#uncomment these later after assigning topic categories for discovered topics
#topic_categories <- c('trade_tarrifs', 'law_speak_us', '') #finish adding them; k == 10
#ts1$topic <- topic_categories[ts1$topic]
#isolate 10 of each topic to view
ts1_topic_sample <- ts1 %>%
group_by(topic) %>%
sample_n(10) %>%
ungroup()
#tests
#identify dist of categories over time (sentiment date and created date)
#isolate differences in mean given the topic [boxplots]
#anova?
View(daily_avg_gain_tidy)
View(trump_dtm)
view(ts1_topic_sample)
#assign topic categories
#get topic distribution for documents
trump_topic_dominant <- topic_dists_trump %>%
group_by(document) %>%
slice_max(gamma, n = 1) %>%
ungroup()
#check if the left join has already occured
if (!("topic" %in% names(ts1))) {
# If the column does not exist, run the left_join
ts1 <- left_join(ts1, trump_topic_dominant, by = c('doc_id' = 'document'))
}
#uncomment these later after assigning topic categories for discovered topics
#topic_categories <- c('trade_tarrifs', 'law_speak_us', '') #finish adding them; k == 10
#ts1$topic <- topic_categories[ts1$topic]
#isolate 10 of each topic to view
ts1_topic_sample <- ts1 %>%
select(created_at, content, text, topic, gamma) %>%
group_by(topic) %>%
sample_n(10) %>%
ungroup()
#assign topic categories
#get topic distribution for documents
trump_topic_dominant <- topic_dists_trump %>%
group_by(document) %>%
slice_max(gamma, n = 1) %>%
ungroup()
#check if the left join has already occured
if (!("topic" %in% names(ts1))) {
# If the column does not exist, run the left_join
ts1 <- left_join(ts1, trump_topic_dominant, by = c('doc_id' = 'document'))
}
#uncomment these later after assigning topic categories for discovered topics
#topic_categories <- c('trade_tarrifs', 'law_speak_us', '') #finish adding them; k == 10
#ts1$topic <- topic_categories[ts1$topic]
#isolate 10 of each topic to view
ts1_topic_sample <- ts1 %>%
select(created_at, content, text, topic, gamma) %>%
group_by(topic) %>%
sample_n(10) %>%
ungroup()
view(ts1_topic_sample)
