{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"c9bZEZRH0rWy","executionInfo":{"status":"ok","timestamp":1742495470833,"user_tz":240,"elapsed":153,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["### Extracting Information from Text\n","\n","## Syntax and structure of a natural language such as English are tied with a set of specific rules, conventions, and principles which dictate how words are combined into phrases, phrases get combined into clauses, and clauses get combined into sentences.\n","## All these constituents exist together in any sentence and are related to each other in a hierarchical structure.\n","## Let’s consider a very basic example of language structure which explains a specific example in the light of subject and predicate relationship. Consider a simple sentence:\n","\n","# Harry is playing football\n","\n","## This sentence is talking about two subjects - Harry and football. To find the subject of the sentence, it is easier to first find the verb and then find “who” or “what” around it.\n","#In the above sentence, “playing” is the verb of predicate.\n","\n","## If you ask “Who is playing?”, the answer is \"Harry\" which gives the first subject, and “What is he playing?” gives us \"football\" as the other subject.\n","#An extensive combination of similar rules allows us to define the entities (subjects), intent (predicates), the relationship between intent and entity, etc.\n","\n","## Such an analysis is very useful in any NLP application since it defines some meaning of the text.\n","#In a collection of words without any relation or structure, it is very difficult to ascertain what it might be trying to convey or what it means.\n","\n","## We'll approach the language syntax and structure problem in 3 parts:\n","\n","# Part of Speech tagging (this tutorial): analyzing syntax of single words\n","# Chunking / shallow parsing (part 2): analyzing multi-word phrases (or chunks) of text\n","# Parsing (part 3): analyzing sentence structure as a whole, and the relation of words to one another"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p3rWfi5x0rW1","executionInfo":{"status":"ok","timestamp":1742503785070,"user_tz":240,"elapsed":150,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"22883632-3863-46dd-c762-cb7105eff52e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('is', 'VERB'), ('quick', 'ADJ'), ('and', 'CONJ'), ('he', 'PRON'), ('is', 'VERB'), ('jumping', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n"]}],"source":["## POS\n","\n","# Parts of speech (POS) tags are specific lexical categories to which words are assigned based on their syntactic context and role.\n","#In English language there are broadly 8 parts of speech: nouns, adjectives, pronouns, interjections, conjunctions, prepositions, adverbs, verbs\n","\n","# For instance, in the sentence:\n","\n","\n","# I am learning NLP\n","\n","# the POS tags are: ('I'/’PRONOUN' 'am'/'VERB' 'learning'/'VERB' 'NLP'/'NOUN')\n","# However, there could be additional detailed tags apart from the generic tags.\n","#In Penn Treebank, a commonly used dataset for language syntax and structure, there are 47 tags defined which are widely used in text analytics and NLP applications.\n","#You can find more information on specific POS tags and their notations at: Penn Treebank Tagset.pdf\n","\n","# The process of classifying and labelling POS tags is called POS tagging.\n","#Let’s try the python example of most commonly used POS tagger using nltk’s pos_tag() function, which is based on the Penn Treebank dataset:\n","\n","import nltk\n","nltk.download('punkt_tab',quiet=True)\n","nltk.download('universal_tagset',quiet=True)\n","nltk.download('averaged_perceptron_tagger_eng',quiet=True)\n","\n","\n","sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n","tokens = nltk.word_tokenize(sentence)\n","tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n","print(tagged_sent)\n","\n","# The preceding output shows us the POS tag for each word in the sentence."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"nnkHDKWT0rW1","executionInfo":{"status":"ok","timestamp":1742495555332,"user_tz":240,"elapsed":6,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["## Implementation of POS taggers\n","# Some prominent approaches used to build a POS tagger are described below:\n","\n","# Rule based\n","# Rule-based tagging is the oldest approach to POS tagging. It uses predefined rules to get possible tags for each word.\n","#The tagger uses information from context (surrounding words) and morphology (within the word), and might also include rules pertaining to such factors as capitalization and punctuation, etc.\n","#A couple of examples are:\n","\n","# If a word X is preceded by a determiner and followed by a noun, tag it as an adjective (contextual rule). Eg, \"The brown fox\".\n","# If a word ends with -ous, tag it as an adjective (morphological rule). Eg, adventurous\n","# An old but useful paper was published by Eric Brill in 1992: A Simple Rule-Based POS tagger. It is the basis of Brill’s Tagger.\n","#See section 2 (about 1 page, easy read) for detailed description of a rule-based POS tagger.\n","\n","# Statistics based\n","# Statistics based taggers have obtained a high accuracy without requiring manually crafted linguistic rules.\n","#There are many methods in statistical model, the most notable for POS tagging being Hidden Markov Models (HMMs) and the maximum entropy approach.\n","#In the HMM model the word-tag probabilities are estimated from a manually annotated corpus (training set).\n","#It is a stochastic model in which the tagger is assumed to be a Markov Process with unobservable states and observable outputs.\n","#Here, the POS tags are the states and the words are the outputs. Hence, the POS tagger consists of:\n","\n","# Ps(Ti): Probability of the sequence starting in tag Ti\n","# Pt(Tj|Ti): Probability of the sequence transitioning from tag Ti to tag Tj\n","# PE(Wj|Ti): Probability of the sequence emitting word Wj on Tag Ti\n","# The Tagger makes two simplifying assumptions:\n","# The probability of a word depends only on its tag, i.e. given its tag, it is independent of other words and other tags.\n","# The probability of a tag depends only on its previous tag, i.e. given the previous tag, it is independent of next tags and tags before the previous tag.\n","# Given a sequence of words, the POS tagger is interested in finding the most likely sequence of tags that generates that sequence of words.\n","\n","# Supervised learning based\n","# The supervised learning based approach to build a POS tagger yields the current most accurate taggers.\n","#It is based on a neural network which is proved to be faster and more accurate than rule based or statistical models.\n","#This approach considers POS tagging as a “supervised learning problem” where manually annotated training data is given to the machine learning model and it learns to predict the missing tags by finding the correlations from the training data.\n","\n","# For example given the predictors (features) as \"POS of word i-1\" or \"last three letters of word at i+1\" etc, can a neural network be trained to predict the \"POS of word i\".\n","#In some sense, it can be looked at as a generalization of the rule based approach, where the supervised learning algorithm is learning the importance of each rule.\n","\n","# Some applications of POS tagging include narrowing down the nouns to focus on the most prominent ones, or performing qualifier-subject analysis, word sense disambiguation, grammar analysis, etc.\n","#The most important use case is to extract phrases from the sentence. In fact, it serves as an input to various more complex analysis such as chunking and parsing."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"pXJqsv2F0rW2","executionInfo":{"status":"ok","timestamp":1742495555805,"user_tz":240,"elapsed":3,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["## Chunking (Shallow Parsing)\n","\n","# Phrasal Structures\n","# A phrase can be a single word or a combination of words based on the syntax and position of the phrase in a clause or sentence. For example, in the following sentence\n","\n","# My dog likes his food.\n","\n","# there are three phrases. \"My dog\" is a noun phrase, \"likes\" is a verb phrase, and \"his food\" is also a noun phrase.\n","# There are five major categories of phrases:\n","# Noun phrase (NP): These are phrases where a noun acts as the head word. Noun phrases act as a subject or object to a verb or an adjective.\n","#In some cases a noun phrase can be replaced by a pronoun without changing the syntax of the sentence. Some examples of Noun phrases are \"little boy\", \"hard rock\", etc.\n","\n","# Verb phrase (VP): These phrases are lexical units that have a verb acting as the head word. Usually there are two forms of verb phrases.\n","#One form has the verb components as well as other entities such as nouns, adjectives, or adverbs as parts of the object. The verb here is known as a finite verb.\n","#For example in the sentence “The boy is playing football”, \"playing football\" is the finite verb phrase.\n","#The second form of this includes verb phrases which consist strictly of verb components only. For example, \"is playing\" in the same sentence is such a verb phrase.\n","\n","# Adjective phrase (ADJP): These are phrases with an adjective as the head word. Their main role is to describe or qualify nouns and pronouns in a sentence, and they will be either placed before or after the noun or pronoun.\n","#The sentence, \"The cat is too cute\" has an adjective phrase, \"too cute\", qualifying \"cat\".\n","\n","# Adverb phrase (ADVP): These are phrases where adverb acts as the head word in the phrase. Adverb phrases are used as modifiers for nouns, verbs, or adverbs themselves by providing further details that describe or qualify them.\n","#In the sentence \"The train should be at the station pretty soon\", the adverb phrase \"pretty soon\" describes when the train would be arriving.\n","\n","# Prepositional phrase (PP): These phrases usually contain a preposition as the head word and other lexical components like nouns, pronouns, and so on.\n","#It acts like an adjective or adverb describing other words or phrases. The phrase \"going up the stairs\" contains a prepositional phrase \"up\", describing the direction of the stairs.\n","\n","# These five major syntactic categories of phrases can be generated from words using several rules, utilizing syntax and grammars of different types."]},{"cell_type":"code","source":["!pip install parse"],"metadata":{"id":"KvVNAril1LiU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":32,"metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"b7-In3500rW2","executionInfo":{"status":"ok","timestamp":1742503830926,"user_tz":240,"elapsed":8,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"35a58fa7-2c71-4fa0-936a-bf32ebca7112"},"outputs":[{"output_type":"stream","name":"stdout","text":["spam\n"]}],"source":["# Shallow parsing, also known as light parsing or chunking, is a technique for analyzing the structure of a sentence in-order to identify these phrases or chunks.\n","#We start by first breaking the sentence down into its smallest constituents (which are tokens such as words) and then grouping them together into higher-level phrases.\n","\n","# In python the parse package uses shallow parsing to extract meaningful chunks out of sentences. The following code snippet shows how to perform shallow parsing on our sample sentence:\n","import parse\n","\n","from parse import *\n","\n","result = parse(\"It's {}, I love it!\", \"It's spam, I love it!\")\n","# print the chunks from shallow parsed sentence tree\n","print(result.fixed[0])\n","\n","# The preceding output is the chunks extracted from the sentence using shallow parsing.\n","#Each line begins with the phrase type, and is followed by the list of words in the phrase along with their part-of-speech tags."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2q9OiXV20rW2","executionInfo":{"status":"ok","timestamp":1742503868029,"user_tz":240,"elapsed":8,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"bb1405ed-0353-40c0-8bbd-b911b2bca8db"},"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP The/DET famous/ADJ algorithm/NOUN)\n","  produced/VERB\n","  (NP accurate/ADJ results/NOUN))\n"]}],"source":["# Let’s take an example of building a basic noun phrase chunker. As explained in the previous section, in a noun phrase, noun acts as a subject or object to a verb or an adjective.\n","#In order to create a noun phrase chunker, we will first define a chunk grammar, consisting of rules that indicate how sentences should be chunked.\n","#For simplicity let’s assume a single rule grammar which says that a noun phrase chunk should be formed whenever the chunker finds an optional determiner (DET) followed by any number of adjectives (ADJ) and then a noun (NOUN):\n","\n","grammar = \"NP: {<DET>?<ADJ>*<NOUN>}\"\n","\n","# Using this grammar, we create a chunk parser, and test it on a sentence. The result is a tree of phrases around noun chunks\n","\n","sentence = \"The famous algorithm produced accurate results\"\n","\n","tokens = nltk.word_tokenize(sentence)\n","tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n","\n","cp = nltk.RegexpParser(grammar)\n","\n","result = cp.parse(tagged_sent)\n","print(result)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"7wPcFim80rW2","executionInfo":{"status":"ok","timestamp":1742503891419,"user_tz":240,"elapsed":41,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["# Likewise you can define multiple grammar rules based on the grammatical phrases you want to extract, for example:\n","grammar = '''\n","    NP: {<DET>? <ADJ>* <NOUN>*}\n","    P: {<PREP>}\n","    V: {<VERB.*>}\n","    PP: {<PREP> <NOUN>}\n","    VP: {<VERB> <NOUN|PREP>*}\n","    '''"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"Vgu9IOBQ0rW3","executionInfo":{"status":"ok","timestamp":1742503896530,"user_tz":240,"elapsed":2559,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["## Machine Learning approach for chunking\n","# Another way to build a Parser is to train a classifier using any commonly used supervised classification algorithms such as SVM, Logistic Regression, etc.\n","# Dataset description\n","# You can use IOB tags as features to train a model that can extract chunks.\n","#In IOB tags, each word is tagged with one of three special chunk tags, I (Inside), O (Outside), or B (Begin). A word is tagged as B if it marks the beginning of a chunk, subsequent words within the chunk are tagged I and all other words are tagged O.\n","\n","# Fortunately, NLTK provides a labelled training corpus to train such a classifier chunker. CoNLL-2000 data consist of three columns.\n","#The first column contains the word, the second its part-of-speech tag and the third its IOB chunk tag. For more details on this corpus, you can read the following paper: Introduction to the CoNLL-2000 Shared Task: Chunking\n","\n","# Let's try an example where we would use CoNLL-2000 corpus to randomly pick 80% of data set for training and remaining 20% to test our classifier.\n","\n","from nltk.corpus import conll2000\n","nltk.download('conll2000',quiet=True)\n","import random\n","\n","conll_data = list(conll2000.chunked_sents())\n","random.shuffle(conll_data)\n","train_sents = conll_data[:int(len(conll_data) * 0.8)]\n","test_sents = conll_data[int(len(conll_data) * 0.8 + 1):]"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"bRJ3quUQ0rW3","executionInfo":{"status":"ok","timestamp":1742503921633,"user_tz":240,"elapsed":1,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["# Defining features\n","# Next, lets define a custom feature extractor we would use to train our model. The features would consist of a sequence of tags based on the co-occurrence of the words with the token.\n","\n","from nltk.stem.porter import PorterStemmer\n","\n","def features(tokens, index, history):\n","    # tokens are tagged words of a sentence\n","    # Index is the index of token for which the features to be extracted\n","    # history is the previous predicted tags\n","\n","    stemmer = PorterStemmer()\n","\n","    # Build the sequence of words for training\n","    tokens = [('__PREVSEQ2__', '__PREVSEQ2__'),\n","        ('__PREVSEQ1__', '__PRESEQ1__')] + list(tokens) + [('__END1__', '__END1__'),\n","        ('__END2__', '__END2__')]\n","    history = ['__PREVSEQ2__', '__PREVSEQ2__'] + list(history)\n","\n","    # shift the index with 2 to point to current token\n","    index += 2\n","\n","    word, pos = tokens[index]\n","    prevword, prevpos = tokens[index - 1]\n","    prev2word, prev2pos = tokens[index - 2]\n","    nextword, nextpos = tokens[index + 1]\n","    next2word, next2pos = tokens[index + 2]\n","\n","    return {\n","        'word': word,\n","        'lemma': stemmer.stem(word),\n","        'pos': pos,\n","\n","        'next-word': nextword,\n","        'next-pos': nextpos,\n","\n","        'next-next-word': next2word,\n","        'next-next-pos': next2pos,\n","\n","        'prev-word': prevword,\n","        'prev-pos': prevpos,\n","\n","        'prev-prev-word': prev2word,\n","        'prev-prev-pos': prev2pos,\n","    }"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzPmA3E50rW3","executionInfo":{"status":"ok","timestamp":1742503944801,"user_tz":240,"elapsed":16070,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"6209d8c9-e02e-4c82-fdcd-7276d4c0b605"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-37-63ad64c620aa>:35: DeprecationWarning: \n","  Function evaluate() has been deprecated.  Use accuracy(gold)\n","  instead.\n","  print(chunker.evaluate(test_sents))\n"]},{"output_type":"stream","name":"stdout","text":["ChunkParse score:\n","    IOB Accuracy:  93.0%%\n","    Precision:     87.5%%\n","    Recall:        90.6%%\n","    F-Measure:     89.0%%\n"]}],"source":["# Training and evaluation\n","# NLTK also provides in its package a sequential tagger that uses a classifier to choose the tag for each token in a sentence.\n","#NLTK's ClassifierBasedTagger can be trained on custom features extracted from CoNLL-2000 or a similar data set.\n","\n","# Let's train NLTK's ClassifierBasedTagger using our custom features on the training data set and evaluate the model on the test sample data.\n","\n","from nltk import ChunkParserI, ClassifierBasedTagger\n","from nltk.chunk import conlltags2tree, tree2conlltags\n","\n","class FooChunkParser(ChunkParserI):\n","    def __init__(self, chunked_sents, **kwargs):\n","\n","        # Transform the trees in IOB annotated sentences [(word, pos, chunk)]\n","        chunked_sents = [tree2conlltags(sent) for sent in chunked_sents]\n","\n","        # Make tags compatible with the tagger interface [((word, pos), chunk)]\n","        def get_tagged_pairs(chunked_sent):\n","            return [((word, pos), chunk) for word, pos, chunk in chunked_sent]\n","\n","        chunked_sents = [get_tagged_pairs(sent) for sent in chunked_sents]\n","\n","        self.feature_detector = features\n","        self.tagger = ClassifierBasedTagger(\n","            train=chunked_sents,\n","            feature_detector=features,\n","            **kwargs)\n","\n","    def parse(self, tagged_sent):\n","        chunks = self.tagger.tag(tagged_sent)\n","        iob_triplets = [(word, token, chunk) for ((word, token), chunk) in chunks]\n","        # Transform the list of triplets to nltk.Tree format\n","        return conlltags2tree(iob_triplets)\n","\n","chunker = FooChunkParser(train_sents)\n","print(chunker.evaluate(test_sents))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqgDR36q0rW3","executionInfo":{"status":"aborted","timestamp":1742495482580,"user_tz":240,"elapsed":11924,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["# Comparison of machine learning vs. rule-based chunker\n","# Different experiments have shown that the performance of the classifier based chunker is very similar to the results obtained by the rule based chunker.\n","#At times it could be considerably hard to define regular expressions to extract chunks which may have very complex structures. In such a cases, the machine learning approach is helpful."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2MRvrMR0rW3","executionInfo":{"status":"aborted","timestamp":1742495482580,"user_tz":240,"elapsed":11923,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["## Deep Parsing\n","# Natural language parsing (also known as deep parsing) is a process of analyzing the complete syntactic structure of a sentence.\n","#This includes how different words in a sentence are related to each other, for example, which words are the subject or object of a verb.\n","#Probabilistic parsing uses language understanding such as grammatical rules.\n","#Alternatively, it may also use supervised training set of hand-parsed sentences to try to infer the most likely syntax and structure of new sentences.\n","\n","# Parsing is used to solve various complex NLP problems such as conversational dialogues and text summarization.\n","#It is different from 'shallow parsing' in that it yields more expressive structural representations which directly capture long-distance dependencies and underlying predicate-argument structures.\n","\n","# There are two main types of parse tree structures - constituency parsing and dependency parsing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWkjG-Ga0rW3","executionInfo":{"status":"aborted","timestamp":1742495482580,"user_tz":240,"elapsed":11923,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["# Constituency Parsing\n","# Constituent-based grammars are used to analyze and determine the components which a sentence is composed of.\n","#There are usually several rules for different types of phrases based on the type of components they can contain, and this can be used to build a parse tree.\n","#The non-terminals nodes in the tree are types of phrases and the terminal nodes are the words in the sentence which are constituents of the phrase.\n","#For example, consider the following sentence and its constituency parse tree\n","\n","\n","# Harry met Sejal\n","#                  Sentence\n","#                     |\n","#       +-------------+------------+\n","#       |                          |\n","#  Noun Phrase                Verb Phrase\n","#       |                          |\n","#     Harry                +-------+--------+\n","#                          |                |\n","#                        Verb          Noun Phrase\n","#                          |                |\n","#                         met             Sejal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WtaBYaS0rW4","executionInfo":{"status":"aborted","timestamp":1742495482581,"user_tz":240,"elapsed":11924,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["# Dependency parsing\n","# The dependency-based grammar is based on the notion that linguistic units, e.g. words, are connected to each other by directed links (one-to-one mappings) between words which signify their dependencies.\n","# The resulting parse tree representation is a labelled directed graph where the nodes are the lexical tokens and the labelled edges show dependency relationships between the heads and their dependents.\n","#The labels on the edges indicate the grammatical role of the dependent. For example, consider the same sentence and its dependency parse tree\n","\n","\n","# Harry met Sejal\n","\n","#               met\n","#                |\n","#        +--------------+\n","#subject |              | subject\n","#      Harry          Sejal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VTMYC-Qc0rW4","executionInfo":{"status":"aborted","timestamp":1742495482610,"user_tz":240,"elapsed":11952,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["# how parsers based on probabilistic context free grammars work. Let’s start by considering the following example sentence:\n","\n","# I went to the market in my shorts.\n","\n","# If we analyze the sentence carefully, there is an ambiguity in phrases - “I went to market” + “in my shorts” and “I went to” + “the market in my shorts”.\n","#While it may to be obvious to us that the first phrasal interpretation is sensible, both phrasal structures are syntactically correct.\n","#The latter would mean there is some market which is in my shorts (analogous to the sentence, \"I went to the market in the city\")."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00wFUUG00rW4","executionInfo":{"status":"ok","timestamp":1742495740172,"user_tz":240,"elapsed":4,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"2f28b533-17cf-43d2-cd2b-562de3baf654"},"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP I)\n","  (VP\n","    (VP (V went) (NP (Det to) (N market)))\n","    (PP (P in) (NP (Det my) (N shorts)))))\n","(S\n","  (NP I)\n","  (VP\n","    (V went)\n","    (NP (Det to) (N market) (PP (P in) (NP (Det my) (N shorts))))))\n"]}],"source":["## Context Free Grammars\n","# A Context Free Grammar (CFG) is a set of rules which can be repeatedly applied to generate a sentence.\n","#Given a sentence and a particular CFG, we can also infer the possible parse tree structures.\n","#Let’s illustrate this with a toy CFG for the above example sentence, and check the possible parse trees:\n","\n","\n","grammar = nltk.CFG.fromstring(\"\"\"S -> NP VP\n","PP -> P NP\n","NP -> Det N | Det N PP | 'I'\n","VP -> V NP | VP PP\n","Det -> 'to' | 'my'\n","N -> 'market' | 'shorts'\n","V -> 'went'\n","P -> 'in'\n","\"\"\")\n","\n","# Here, S is the start symbol, NP, VP, etc represent noun phrase, verb phrase, etc, and N, V, etc represent noun, verb, etc.\n","# This grammar permits the sentence to be analyzed in two ways, depending on whether the prepositional phrase \"in my shorts\" describes the subject \"market\" or going to market.\n","\n","import nltk\n","sent = 'I went to market in my shorts'.split()\n","parser = nltk.ChartParser(grammar)\n","for tree in parser.parse(sent):\n","    print(tree)"]},{"cell_type":"code","source":["sentence = 'The dog saw a man in the park'\n","\n","grammar = nltk.CFG.fromstring('''S -> NP VP\n","VP -> V NP | V NP PP\n","PP -> P NP\n","NP -> Det N | Det N PP | 'I'\n","Det -> 'a' | 'the'\n","N -> 'man' | 'dog' | 'park'\n","V -> 'saw'\n","P -> 'in'\n","''')\n","\n","# Here, S is the start symbol, NP, VP, etc represent noun phrase, verb phrase, etc, and N, V, etc represent noun, verb, etc.\n","# This grammar permits the sentence to be analyzed in two ways, depending on whether the prepositional phrase \"in my shorts\" describes the subject \"market\" or going to market.\n","\n","import nltk\n","sent = sentence.lower().split()\n","parser = nltk.ChartParser(grammar)\n","for tree in parser.parse(sent):\n","    print(tree)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5FmWvGSXAn-","executionInfo":{"status":"ok","timestamp":1742504544295,"user_tz":240,"elapsed":11,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"c23f8f50-c2b9-470f-8110-36a008f8dd16"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP (Det the) (N dog))\n","  (VP\n","    (V saw)\n","    (NP (Det a) (N man))\n","    (PP (P in) (NP (Det the) (N park)))))\n","(S\n","  (NP (Det the) (N dog))\n","  (VP\n","    (V saw)\n","    (NP (Det a) (N man) (PP (P in) (NP (Det the) (N park))))))\n"]}]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HfpYyfoc0rW4","executionInfo":{"status":"ok","timestamp":1742495749776,"user_tz":240,"elapsed":439,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"6844508c-90dd-429a-991b-317276c9e22d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n"]},{"output_type":"stream","name":"stdout","text":["(S\n","  (NP-SBJ (NNP Mr.) (NNP Vinken))\n","  (VP\n","    (VBZ is)\n","    (NP-PRD\n","      (NP (NN chairman))\n","      (PP\n","        (IN of)\n","        (NP\n","          (NP (NNP Elsevier) (NNP N.V.))\n","          (, ,)\n","          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n","  (. .))\n"]}],"source":["## Probabilistic Context Free Grammars\n","# In-order to disambiguate between the above possible trees, we can use a Probabilistic Context Free Grammar (PCFG).\n","#In this setting, each of the rules are probabilistic.\n","#For example, if we take the rule N -> 'market' | 'shorts', our grammar would specify with what probabilities corresponding to 'market' and 'shorts'.\n","#In general, the probabilities and rules are inferred from annotated datasets.\n","\n","# Now, we will build a custom constituency parsers by creating our own PCFG rules and then using NLTK’s ViterbiParser to train a parser.\n","#We will use treebank corpus that provides annotated parse trees for sentences.\n","\n","nltk.download('treebank',quiet=True)\n","\n","from nltk.grammar import Nonterminal\n","from nltk.corpus import treebank\n","\n","# get training data\n","training_set = treebank.parsed_sents()\n","\n","# example training sentence\n","print(training_set[1])"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HD9ONAfz0rW5","executionInfo":{"status":"ok","timestamp":1742495755128,"user_tz":240,"elapsed":2577,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"c4253ed5-c916-489c-e74f-d467187f5f52"},"outputs":[{"output_type":"stream","name":"stdout","text":["[NN -> 'movie', NN -> 'Dividend', NNS -> 'six-packs', NN -> 'low-altitude', NNS -> 'exports']\n"]}],"source":["# Next, we will build the rules for our grammar by extracting them from the annotated training sentences.\n","# Extract the rules for all annotated training sentences\n","rules = list(set(rule for sent in training_set for rule in sent.productions()))\n","print(rules[0:5])"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"pDh1EEg60rW5","executionInfo":{"status":"ok","timestamp":1742495741243,"user_tz":240,"elapsed":7,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}