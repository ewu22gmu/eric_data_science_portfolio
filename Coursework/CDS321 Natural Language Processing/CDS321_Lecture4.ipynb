{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ohYX8-kyhO16"},"outputs":[],"source":["### Processing Raw Text\n","\n","## Let's try on some things first:\n","## convert text to lowercase - notice that you don't need any modules for this\n","\n","\n","input_str = 'The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.'\n","input_str = input_str.lower()\n","print(input_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdwMDOsZhO17","outputId":"592e3b94-1fa3-4a06-ea35-d871cc07646b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"]}],"source":["## let's remove numbers in a text\n","\n","import re\n","input_str = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n","result = re.sub(r'\\d+', '', input_str)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"4b4DtRdrhO17","outputId":"b90529cb-e489-402b-d357-e1d6be75df1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is an example of string with punctuation\n"]}],"source":["## let's remove punctuation\n","\n","## In simple terms, maketrans() method is a static method that creates a one to one mapping of a character to its translation/replacement.\n","## It creates a Unicode representation of each character for translation.\n","## This translation mapping is then used for replacing a character to its mapped character when used in translate() method.\n","\n","import string\n","input_str = 'This &is [an] example? {of} string. with.? punctuation!!!!'\n","result = input_str.translate(str.maketrans('','', string.punctuation))\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75Lx_fKuhO18","outputId":"119c9056-66b3-4d5b-bc6c-e1b39b38180c"},"outputs":[{"name":"stdout","output_type":"stream","text":["th3s 3s str3ng 2x1mpl2....w4w!!!\n"]}],"source":["## another example of the use of translate()\n","\n","intab = \"aeiou\"\n","outtab = \"12345\"\n","trantab = str.maketrans(intab, outtab)\n","\n","str = \"this is string example....wow!!!\"\n","print (str.translate(trantab))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCi36h-ahO18","outputId":"95e1a98a-4547-4e0c-f56e-9d9f78072e44"},"outputs":[{"data":{"text/plain":["'a string example'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["## remove white spaces\n","\n","input_str = ' \\t a string example\\t '\n","input_str = input_str.strip()\n","input_str"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"i09YYadrhO18","outputId":"d4b9ae08-debe-4776-c184-cfb4d93f23d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"]}],"source":["## remove stop words\n","\n","import nltk\n","from nltk.corpus import stopwords\n","\n","input_str = 'NLTK is a leading platform for building Python programs to work with human language data.'\n","stop_words = set(stopwords.words('english'))\n","from nltk.tokenize import word_tokenize\n","tokens = word_tokenize(input_str)\n","result = [i for i in tokens if not i in stop_words]\n","print (result)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"YS28_oIyhO18"},"outputs":[],"source":["## remove stopwords without NLTK\n","\n","import spacy ## use pip install or conda install if you don't already have it imported\n","import sklearn\n","\n","from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n","from spacy.lang.en.stop_words import STOP_WORDS"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"fOL9GqlKhO18","outputId":"3350f320-faaa-4587-ed43-6a3f985ab092"},"outputs":[{"name":"stdout","output_type":"stream","text":["there\n","are\n","sever\n","type\n","of\n","stem\n","algorithm\n",".\n"]}],"source":["## stemming\n","\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","stemmer= PorterStemmer()\n","input_str='There are several types of stemming algorithms.'\n","input_str=word_tokenize(input_str)\n","for word in input_str:\n","    print(stemmer.stem(word))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-OtJhQwhO18","outputId":"96b8a302-6069-4418-eb5c-bd7191bc66e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["been\n","had\n","done\n","language\n","city\n","mouse\n"]}],"source":["## lemmatization\n","\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","lemmatizer=WordNetLemmatizer()\n","input_str= 'been had done languages cities mice'\n","input_str=word_tokenize(input_str)\n","for word in input_str:\n","    print(lemmatizer.lemmatize(word))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dh4k1VB6hO18","outputId":"09a1ce8a-97e3-4eaa-d711-af7875d9b7a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"]}],"source":["## parts of speech (POS)\n","\n","import textblob ## use pip install or conda install if you don't already have it imported\n","\n","input_str= 'Parts of speech examples: an article, to write, interesting, easily, and, of'\n","from textblob import TextBlob\n","result = TextBlob(input_str)\n","print(result.tags)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzeqEXz6hO18","outputId":"05d74f19-b9bc-405c-ecf8-1f0fe3e657b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('A', 'DT'), ('black', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('a', 'DT'), ('white', 'JJ'), ('stove', 'NN'), ('were', 'VBD'), ('bought', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('apartment', 'NN'), ('of', 'IN'), ('John', 'NNP')]\n","(S\n","  (NP A/DT black/JJ television/NN)\n","  and/CC\n","  (NP a/DT white/JJ stove/NN)\n","  were/VBD\n","  bought/VBN\n","  for/IN\n","  (NP the/DT new/JJ apartment/NN)\n","  of/IN\n","  John/NNP)\n"]}],"source":["## chunking (shallow parsing)\n","\n","input_str= 'A black television and a white stove were bought for the new apartment of John.'\n","from textblob import TextBlob\n","result = TextBlob(input_str)\n","print(result.tags) ## run these separately\n","\n","reg_exp = 'NP: {<DT>?<JJ>*<NN>}'\n","rp = nltk.RegexpParser(reg_exp)\n","result = rp.parse(result.tags)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"CoVNOoxAhO18","outputId":"9d05bdfd-f790-4a40-e09b-9027d4aa2ee7"},"outputs":[{"name":"stdout","output_type":"stream","text":["(S\n","  (PERSON Bill/NNP)\n","  works/VBZ\n","  for/IN\n","  Apple/NNP\n","  so/IN\n","  he/PRP\n","  went/VBD\n","  to/TO\n","  (GPE Boston/NNP)\n","  for/IN\n","  a/DT\n","  conference/NN\n","  ./.)\n"]}],"source":["## named entity recognition\n","\n","from nltk import word_tokenize, pos_tag, ne_chunk\n","input_str = 'Bill works for Apple so he went to Boston for a conference.'\n","print (ne_chunk(pos_tag(word_tokenize(input_str))))"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"jtK8iwRghO19","executionInfo":{"status":"ok","timestamp":1739554805687,"user_tz":300,"elapsed":9783,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"outputs":[],"source":["from __future__ import division ## in your textbook this is specified for python2 users only, but actually it is working with python3 as well\n","import nltk, re, pprint\n","from nltk import word_tokenize"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"koFo6mEehO19","executionInfo":{"status":"ok","timestamp":1739554806097,"user_tz":300,"elapsed":415,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"a46a6650-1db9-4326-830e-c3faf8b761de","colab":{"base_uri":"https://localhost:8080/","height":34}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["### Let's look at electronic books\n","\n","from urllib import request\n","url = 'http://www.gutenberg.org/files/2554/2554-0.txt'\n","response = request.urlopen(url)\n","raw = response.read().decode('utf8')\n","type(raw)  ## you need to run each of these separately\n","\n","len(raw)\n","\n","raw[:75]\n","\n","## why is this output \\ufeff and \\r ?"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QoLmJgbvhO19","executionInfo":{"status":"error","timestamp":1739554809042,"user_tz":300,"elapsed":656,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"e1e7ffe7-ef7b-4f6e-eda3-a9184add2979","colab":{"base_uri":"https://localhost:8080/","height":687}},"outputs":[{"output_type":"error","ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-5e827f12a74e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## This step is called tokenization, and it produces our familiar structure, a list of words and punctuation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## make sure you run each of these separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}],"source":["## we want to break up the string into words and punctuation, as we saw above.\n","## This step is called tokenization, and it produces our familiar structure, a list of words and punctuation.\n","\n","tokens = word_tokenize(raw) ## make sure you run each of these separately\n","\n","len(tokens)\n","\n","tokens[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ELNPt00HhO19","outputId":"56c00d5b-f148-4001-a445-9523841bf970"},"outputs":[{"data":{"text/plain":["['an',\n"," 'exceptionally',\n"," 'hot',\n"," 'evening',\n"," 'early',\n"," 'in',\n"," 'July',\n"," 'a',\n"," 'young',\n"," 'man',\n"," 'came',\n"," 'out',\n"," 'of',\n"," 'the',\n"," 'garret',\n"," 'in',\n"," 'which',\n"," 'he',\n"," 'lodged',\n"," 'in',\n"," 'S.',\n"," 'Place',\n"," 'and',\n"," 'walked',\n"," 'slowly',\n"," ',',\n"," 'as',\n"," 'though',\n"," 'in',\n"," 'hesitation',\n"," ',',\n"," 'towards',\n"," 'K.',\n"," 'bridge',\n"," '.',\n"," 'He',\n"," 'had',\n"," 'successfully']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["text = nltk.Text(tokens)\n","\n","text[1024:1062]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeEhuYBHhO19"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}