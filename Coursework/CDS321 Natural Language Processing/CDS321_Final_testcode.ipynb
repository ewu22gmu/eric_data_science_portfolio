{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3e46NJor5Y161S5xxL7ot"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## EDA + basic nlp"],"metadata":{"id":"fFKySfSCBsMU"}},{"cell_type":"code","source":["import spacy\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.probability import FreqDist\n","from collections import Counter\n","\n","# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('vader_lexicon')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Load SpaCy model - medium model with word vectors\n","nlp = spacy.load(\"en_core_web_md\")"],"metadata":{"id":"d1sRmZAjAgfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analyze_sentiment(text):\n","  '''\n","    Analyzes overall sentiment, sentiment per line, and changes in sentiment of the poem\n","  '''\n","  sia = SentimentIntensityAnalyzer()\n","\n","  #Overall sentiment\n","  overall_sentiment = sia.polarity_scores(text)\n","\n","  #Sentiment by line\n","  line_sentiment = list()\n","  lines = text.split('\\n')\n","\n","  for i, line in enumerate(lines):\n","    sentiment = sia.polarity_scores(line)\n","    line_sentiment.append({'linenum': i+1,\n","                           'text': line,\n","                           'sentiment': sentiment})\n","\n","  #Changes in sentiment\n","  sentiment_shift = list()\n","  prev = None\n","  for i,data in enumerate(line_sentiment):\n","    current = data['sentiment']['compound']\n","    if prev is not None:\n","      shift = current - prev\n","      if abs(shift) > 0.3:\n","        sentiment_shift.append({'from line': i,\n","                                'to line': i+1,\n","                                'shift': shift})\n","    prev = current\n","\n","\n","  return {'overall': overall_sentiment,\n","          'by line': line_sentiment,\n","          'sentiment shift': sentiment_shift,\n","          'primary sentiment': \"positive\" if overall_sentiment[\"compound\"] > 0.05\n","                          else \"negative\" if overall_sentiment[\"compound\"] < -0.05\n","                          else \"neutral\"}"],"metadata":{"id":"P1IEXzAJ7NJO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_entities(text):\n","  '''\n","    Gets entities from text via spacy\n","  '''\n","\n","  poem = nlp(text)\n","  entities = [(ent.text, ent.label_) for ent in poem.ents]\n","  return entities"],"metadata":{"id":"swfGAcYdAYcq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analyze_df(df, poem='strip'):\n","  '''\n","    Analyzes a df of poems\n","\n","    inputs:\n","      df: dataframe\n","      poem: column name of the poem\n","\n","    returns:\n","      df: dataframe with additional columns for analysis\n","  '''\n","\n","  results = list() #store results\n","\n","  for ndx, row in df.iterrows():\n","    pass\n","\n","  return print()"],"metadata":{"id":"xyAsy3Mb-qGg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformer model test"],"metadata":{"id":"9x7I1tot-0BD"}},{"cell_type":"code","source":["#Import packages for implementing the Hugging Face Transformer Model\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    Trainer,\n","    TrainingArguments\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder"],"metadata":{"id":"BLv5BNrkHaOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PoemDataset(Dataset):\n","    def __init__(self, poems, labels, tokenizer, max_length=512):\n","        \"\"\"\n","        Custom dataset for poem meaning extraction\n","\n","        Args:\n","            poems (list): List of poems (each poem is a list of lines)\n","            labels (list): Corresponding labels for poems\n","            tokenizer: Hugging Face tokenizer\n","            max_length (int): Maximum sequence length for tokenization\n","        \"\"\"\n","        self.poems = poems\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.poems)\n","\n","    def __getitem__(self, idx):\n","        # Convert poem lines to a single string\n","        poem_text = ' '.join(self.poems[idx])\n","\n","        # Tokenize the poem\n","        encoding = self.tokenizer(\n","            poem_text,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n","        }"],"metadata":{"id":"iJ2MukPH47JP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_poem_meanings(df):\n","    \"\"\"\n","    Main function to fine-tune a transformer model for poem meaning extraction\n","\n","    Args:\n","        df (pd.DataFrame): DataFrame containing poems as lists of lines\n","\n","    Returns:\n","        Trained model and tokenizer for inference\n","    \"\"\"\n","    # Unsupervised approach for creating pseudo-labels\n","    def generate_pseudo_labels(poems):\n","        \"\"\"\n","        Generate pseudo-labels using unsupervised techniques\n","\n","        Strategies:\n","        1. Sentiment analysis\n","        2. Thematic clustering\n","        3. Topic modeling\n","        \"\"\"\n","        # Example: Simple sentiment-based pseudo-labeling\n","        from textblob import TextBlob\n","\n","        def get_sentiment_label(poem):\n","            # Convert poem lines to a single text\n","            poem_text = ' '.join(poem)\n","\n","            # Use TextBlob for sentiment analysis\n","            sentiment = TextBlob(poem_text).sentiment.polarity\n","\n","            # Categorize sentiment into discrete labels\n","            if sentiment > 0.5:\n","                return 2  # Very Positive\n","            elif sentiment > 0:\n","                return 1  # Positive\n","            elif sentiment < -0.5:\n","                return 4  # Very Negative\n","            elif sentiment < 0:\n","                return 3  # Negative\n","            else:\n","                return 0  # Neutral\n","\n","        return [get_sentiment_label(poem) for poem in poems]\n","\n","    # Prepare data\n","    poems = df['poems'].tolist()\n","\n","    # Generate pseudo-labels\n","    labels = generate_pseudo_labels(poems)\n","\n","    # Split the data\n","    train_poems, val_poems, train_labels, val_labels = train_test_split(\n","        poems, labels, test_size=0.2, random_state=42\n","    )\n","\n","    # Load pre-trained model and tokenizer\n","    model_name = \"distilbert-base-uncased\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_name,\n","        num_labels=len(set(labels))\n","    )\n","\n","    # Create datasets\n","    train_dataset = PoemDataset(train_poems, train_labels, tokenizer)\n","    val_dataset = PoemDataset(val_poems, val_labels, tokenizer)\n","\n","    # Training arguments\n","    training_args = TrainingArguments(\n","        output_dir='./poem_meaning_model',\n","        num_train_epochs=3,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=16,\n","        warmup_steps=500,\n","        weight_decay=0.01,\n","        logging_dir='./logs',\n","        logging_steps=10,\n","        evaluation_strategy=\"epoch\"\n","    )\n","\n","    # Initialize Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset\n","    )\n","\n","    # Train the model\n","    trainer.train()\n","\n","    # Save the model\n","    model.save_pretrained('./poem_meaning_model')\n","    tokenizer.save_pretrained('./poem_meaning_model')\n","\n","    return model, tokenizer"],"metadata":{"id":"tAdqbB-24CCy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inference_poem_meanings(model, tokenizer, poems):\n","    \"\"\"\n","    Perform inference on new poems\n","\n","    Args:\n","        model: Fine-tuned transformer model\n","        tokenizer: Corresponding tokenizer\n","        poems (list): List of poems to extract meanings from\n","\n","    Returns:\n","        List of predicted meaning labels\n","    \"\"\"\n","    model.eval()\n","\n","    # Mapping of labels (customize based on your pseudo-labeling)\n","    label_mapping = {\n","        0: 'Neutral',\n","        1: 'Positive',\n","        2: 'Very Positive',\n","        3: 'Negative',\n","        4: 'Very Negative'\n","    }\n","\n","    predictions = []\n","\n","    for poem in poems:\n","        poem_text = ' '.join(poem)\n","        inputs = tokenizer(\n","            poem_text,\n","            truncation=True,\n","            padding=True,\n","            return_tensors='pt'\n","        )\n","\n","        #Check if CUDA Acceleration is available\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        #Select available device for model to run on\n","        inputs = inputs.to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            predicted_label = torch.argmax(outputs.logits, dim=1).item()\n","            predictions.append(label_mapping[predicted_label])\n","\n","    return predictions"],"metadata":{"id":"4o_nYRp84LXz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","def main():\n","    #load in data frame and split data\n","    train_df, test_df = train_test_split(ECPA_df, test_size=2440, random_state=42, stratify=None)\n","\n","    #rename lines to poems\n","    train_df = train_df.rename(columns={'lines': 'poems'})\n","    test_df = test_df.rename(columns={'lines': 'poems'})\n","\n","    model, tokenizer = extract_poem_meanings(train_df)\n","\n","    #perform inference\n","    meanings = inference_poem_meanings(model, tokenizer, test_df)\n","    print(meanings)\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","#API Key: 8f7e9305d2ef84b9c17e30cd3a661dec0d39e87a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":268},"id":"iasHQvO94O7O","executionInfo":{"status":"ok","timestamp":1741189347439,"user_tz":300,"elapsed":148556,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"96905ebb-0005-4c1a-f9b1-c8cf07162aad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [315/315 02:23, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.563700</td>\n","      <td>0.451061</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.536400</td>\n","      <td>0.423291</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.440400</td>\n","      <td>0.434250</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["['Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"o_Mp7CxrBqLt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Topic Modeling"],"metadata":{"id":"cL6luk0869en"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation"],"metadata":{"id":"uMaX3NvC6_W8","executionInfo":{"status":"ok","timestamp":1741205300958,"user_tz":300,"elapsed":13139,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Sample poem data (replace with your actual data)\n","poems = [\n","    \"The sun is setting, casting shadows long,\\nA lonely bird sings a melancholic song.\",\n","    \"A gentle breeze whispers through the trees,\\nThe river flows, carrying memories.\",\n","    \"In fields of gold, where poppies bloom,\\nLove's sweet scent fills every room.\",\n","    \"The storm rages, thunder roars,\\nFear grips the heart, as darkness pours.\"\n","]\n","\n","# Candidate themes (replace with your list)\n","candidate_themes = [\n","    \"love\", \"death\", \"nature\", \"time\", \"religion\", \"war\", \"identity\",\n","    \"isolation\", \"hope\", \"loss\", \"memory\", \"freedom\", \"politics\",\n","    \"beauty\", \"struggle\", \"mortality\", \"spirituality\", \"childhood\",\n","    \"resilience\", \"transformation\", \"nostalgia\", \"passion\", \"sorrow\"\n","]\n","\n","# Preprocessing function\n","def preprocess_text(text):\n","    text = text.lower()\n","    tokens = word_tokenize(text)\n","    stop_words = set(stopwords.words('english'))\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n","    return \" \".join(tokens)\n","\n","processed_poems = [preprocess_text(poem) for poem in poems]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"CAy1SA0o7Btw","executionInfo":{"status":"error","timestamp":1741205363668,"user_tz":300,"elapsed":5,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"5ecd3248-7fb3-4419-85b9-cccc11b64db7"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'nltk' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-878654303b5c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Sample poem data (replace with your actual data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"]}]},{"cell_type":"code","source":["vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n","dtm = vectorizer.fit_transform(processed_poems)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":151},"id":"VSaey4Eq7G0n","executionInfo":{"status":"error","timestamp":1741205315086,"user_tz":300,"elapsed":44,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"9a46e41f-968a-473a-bec8-cc8f2a5f334a"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'processed_poems' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-e9e5133d24f1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_poems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'processed_poems' is not defined"]}]},{"cell_type":"code","source":["num_topics = len(candidate_themes) #set number of topics to the number of candidate themes.\n","lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n","lda.fit(dtm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168},"id":"4GPv36w97J19","executionInfo":{"status":"error","timestamp":1741205364986,"user_tz":300,"elapsed":17,"user":{"displayName":"Eric Wu","userId":"12657333118463688685"}},"outputId":"f388f1f1-2a0d-4139-ec1e-71ed38562f69"},"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'candidate_themes' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-f4d1276b34b1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_themes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#set number of topics to the number of candidate themes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'candidate_themes' is not defined"]}]},{"cell_type":"code","source":["feature_names = vectorizer.get_feature_names_out()\n","\n","def get_top_words(model, feature_names, n_top_words):\n","    topics = []\n","    for topic_idx, topic in enumerate(model.components_):\n","        top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n","        top_words = [feature_names[i] for i in top_words_idx]\n","        topics.append(top_words)\n","    return topics\n","\n","top_words_per_topic = get_top_words(lda, feature_names, 5) #get 5 top words per topic.\n","\n","# Match topics to candidate themes (simplest approach: keyword overlap)\n","theme_matches = {}\n","\n","for i, topic_words in enumerate(top_words_per_topic):\n","    best_match = None\n","    best_overlap = 0\n","    for theme in candidate_themes:\n","        overlap = len(set(topic_words) & {theme}) #check if the candidate theme word itself is in the top words.\n","        if overlap > best_overlap:\n","            best_overlap = overlap\n","            best_match = theme\n","        else:\n","            for word in topic_words: #check if any of the top words are semantically related to the candidate theme.\n","                if theme in word: #crude way to check similarity, can be improved with word embeddings.\n","                    best_overlap = 1\n","                    best_match = theme\n","    theme_matches[f\"Topic {i+1}\"] = best_match\n","\n","print(\"Topic Words:\")\n","for i, words in enumerate(top_words_per_topic):\n","    print(f\"Topic {i+1}: {', '.join(words)}\")\n","\n","print(\"\\nTheme Matches:\")\n","print(theme_matches)\n","\n","#Assign the themes to each poem.\n","poem_themes = []\n","doc_topic_dist = lda.transform(dtm)\n","\n","for doc_index, dist in enumerate(doc_topic_dist):\n","    topic_index = dist.argmax()\n","    poem_themes.append(theme_matches[f\"Topic {topic_index + 1}\"])\n","\n","poem_theme_df = pd.DataFrame({'poem': poems, 'theme': poem_themes})\n","print('\\nPoem Themes:')\n","print(poem_theme_df)"],"metadata":{"id":"5ptXzduD7WB7"},"execution_count":null,"outputs":[]}]}